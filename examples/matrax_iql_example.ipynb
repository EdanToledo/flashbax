{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using IQL and Flashbax in Matrax\n",
    "\n",
    "This guide demonstrates how to use the Item Buffer for experience replay in reinforcement learning tasks. Specifically, we implement independent Q-learning ([original IQL paper](https://arxiv.org/pdf/1511.08779.pdf)), using a single Q-network to solve two-player matrix games hosted in [Matrax](https://github.com/instadeepai/matrax?tab=readme-ov-file). The Item Buffer operates by saving all experience data in a first-in-first-out (FIFO) queue and returns batches of uniformly sampled experience from it. The point of the item buffer is for a simplified buffer experience where each data item stored is completely independent of each other. An example of this would be full state-action-reward-nextstate transitions for Q Learning. Additionally, as other buffers generally maintain a temporal relation with their data, the buffers have restrictions on their size depending on whether data is added in batches or not; but, since there is no temporal relation to the item buffer, the exact size of a desired buffer is achievable regardless of whether data is added in batches, sequences or both batches of sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we briefly explain the Matrax environment and how different games work:\n",
    "\n",
    "### The Matrax Environment\n",
    "\n",
    "*Two-player matrix games in JAX*\n",
    "\n",
    "A matrix game is a two-player game where each player has a set of actions and a payoff matrix. The payoff matrix is a two-dimensional array where the rows correspond to the actions of Player 1 and the columns correspond to the actions of Player 2. The entry at row $i$ and column $j$ for Player 1 is the reward given to Player 1 when playing action $i$ and Player 2 plays action $j$. Similarly, the entry at row $i$ and column $j$ for Player 2 is the reward given to Player 2 when playing action $j$ and Player 1 plays action $i$.\n",
    "\n",
    "Maximum number of steps defaults to 500.\n",
    "\n",
    "#### üîª Penalty Game\n",
    "- **Shape (action space):** 3 $\\times$ 3\n",
    "- **Registered versions:** Penalty-{k}-{state}-v0\n",
    "- **Valid arguments:** $k \\in \\{0, 25, 50, 75, 100\\}$\n",
    "- **Payoff matrix (for each agent):**\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix} \n",
    "-k & 0 & 10 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "10 & 0 & -k \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "#### üßó‚Äç‚ôÄÔ∏è Climbing Game\n",
    "- **Shape (action space):** 3 $\\times$ 3\n",
    "- **Registered versions:** Climbing-{state}-v0\n",
    "- **Payoff matrix (for each agent):**\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix} \n",
    "11 & -30 & 0 \\\\\n",
    "-30 & 7 & 0 \\\\\n",
    "0 & 6 & 5 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "#### ü§ù No-Conflict Games\n",
    "- **Shape (action space):** 2 $\\times$ 2\n",
    "- **Registered versions:** NoConflict-{id}-{state}-v0\n",
    "- **Valid arguments:** $\\texttt{id} \\in \\{0, 1, 2, ..., 20\\}$\n",
    "- **Payoff matrix:** controlled by $\\texttt{id}$ as dictated [here](https://github.com/instadeepai/matrax/blob/main/matrax/games/conflict.py).\n",
    "\n",
    "#### üí£ Conflict Games\n",
    "- **Shape (action space):** 2 $\\times$ 2\n",
    "- **Registered versions:** Conflict-{id}-{state}-v0\n",
    "- **Valid arguments:** $\\texttt{id} \\in \\{0, 1, 2, ..., 56\\}$\n",
    "- **Payoff matrix:** controlled by $\\texttt{id}$ as dictated [here](https://github.com/instadeepai/matrax/blob/main/matrax/games/no_conflict.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install flashbax matrax jumanji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import matrax\n",
    "from jumanji.wrappers import AutoResetWrapper\n",
    "import collections\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "import flashbax as fbx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we have chosen a No Conflict game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a matrix game environment\n",
    "ENV_NAME = \"NoConflict-0-stateless-v0\"\n",
    "\n",
    "# Enable auto-resetting of the training environment\n",
    "env = AutoResetWrapper(matrax.make(ENV_NAME))\n",
    "\n",
    "# Leave evaluation environment without auto-resetting\n",
    "eval_env = matrax.make(ENV_NAME)\n",
    "\n",
    "NUM_ACTIONS = env.num_actions\n",
    "NUM_AGENTS = env.num_agents\n",
    "NUM_OBS = NUM_AGENTS  # in matrax, observations have shape (num_agents, num_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific matrix game has 2 agents and 2 actions, so each agents payoff matrix will be 2x2 in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Number of actions: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of agents:\", env.num_agents)\n",
    "print(\"Number of actions:\", env.num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the agents' payoff matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payoff Matrices:\n",
      "\n",
      "Agent 1 Payoff Matrix:\n",
      "[  4   3  ]\n",
      "[  2   1  ]\n",
      "\n",
      "\n",
      "Agent 2 Payoff Matrix:\n",
      "[  4   3  ]\n",
      "[  2   1  ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Payoff Matrices:\\n\")\n",
    "for agent in range(NUM_AGENTS):\n",
    "    print(f\"Agent {agent + 1} Payoff Matrix:\")\n",
    "    for row in env.payoff_matrix[agent]:\n",
    "        print(f\"[{row[0]:3.0f} {row[1]:3.0f}{' ' * 2}]\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup IQL Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]  # assuming batch size is leading dim\n",
    "\n",
    "        return x.reshape(batch_size, -1)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    num_actions: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, state):\n",
    "        return nn.Sequential(\n",
    "            [\n",
    "                Flatten(),\n",
    "                nn.Dense(20),\n",
    "                nn.relu,\n",
    "                nn.Dense(20),\n",
    "                nn.relu,\n",
    "                nn.Dense(self.num_actions),\n",
    "            ]\n",
    "        )(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Instantiate Q-network\n",
    "q_network = QNetwork(NUM_ACTIONS)\n",
    "\n",
    "# Create a single dummy observation (i.e., batch size is 1)\n",
    "# We add num_agents to num_obs to account for the addition of one-hot-encoded agent IDs\n",
    "dummy_obs = jnp.zeros((1, NUM_OBS + NUM_AGENTS), jnp.float32)\n",
    "\n",
    "# Generate random key for initialising params\n",
    "key = jax.random.PRNGKey(SEED)\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "# Initialise Q-network\n",
    "q_network_params = q_network.init(subkey, dummy_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store online and target parameters\n",
    "QLearnParams = collections.namedtuple(\"Params\", [\"online\", \"target\"])\n",
    "\n",
    "# Q-learn-state\n",
    "QLearnState = collections.namedtuple(\"LearnerState\", [\"count\", \"optim_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_greedy_action(q_values):\n",
    "    \"\"\"A function to select the action corresponding to the largest inputted Q-value for each agent.\"\"\"\n",
    "\n",
    "    action = jnp.argmax(q_values)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_action(subkey):\n",
    "    \"\"\"A function to select an action randomly for each agent.\"\"\"\n",
    "\n",
    "    action = jax.random.randint(subkey, shape=(), minval=0, maxval=NUM_ACTIONS)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_DECAY_TIMESTEPS = 50_000  # decay epsilon over 50,000 timesteps\n",
    "EPSILON_MIN = 0.1  # 10% exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(num_timesteps):\n",
    "    \"\"\"A function to retrieve the value of epsilon, based on a (linear) decay rate and a minimum possible value.\"\"\"\n",
    "\n",
    "    epsilon = 1.0 - num_timesteps / EPSILON_DECAY_TIMESTEPS\n",
    "\n",
    "    epsilon = jax.lax.select(epsilon < EPSILON_MIN, EPSILON_MIN, epsilon)\n",
    "\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(key, q_values, num_timesteps):\n",
    "    \"\"\"A function to perform epsilon-greedy action selection.\"\"\"\n",
    "\n",
    "    epsilon = get_epsilon(num_timesteps)\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "\n",
    "    should_explore = jax.random.uniform(subkey, (1,))[0] < epsilon\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "\n",
    "    action = jax.lax.select(\n",
    "        should_explore, select_random_action(subkey), select_greedy_action(q_values)\n",
    "    )\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_select_action(key, params, num_timesteps, obs, evaluation=False):\n",
    "    \"\"\"A function to perform greedy or epsilon-greedy action selection,\n",
    "    based on whether we are evaluating or training a policy.\n",
    "    \"\"\"\n",
    "\n",
    "    obs = jnp.expand_dims(obs, axis=0)  # add a batch dim as the leading axis\n",
    "\n",
    "    q_values = q_network.apply(params.online, obs)[0]  # remove batch dim\n",
    "\n",
    "    action = select_epsilon_greedy_action(key, q_values, num_timesteps)\n",
    "    greedy_action = select_greedy_action(q_values)\n",
    "\n",
    "    action = jax.lax.select(evaluation, greedy_action, action)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Item Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the item buffer and instantiate it with a full environment transition consisting of all agent observations, actions, rewards, next state observations and a global done flag. The buffer is used to sample batches of these transitions to perform Q-learning for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the replay buffer\n",
    "BATCH_SIZE = 64\n",
    "q_learning_memory = fbx.make_item_buffer(\n",
    "    max_length=50_000, min_length=64, sample_batch_size=BATCH_SIZE, add_batches=True\n",
    ")\n",
    "\n",
    "# Make a dummy observation and initialise the replay buffer\n",
    "transition = {\n",
    "    \"obs\": jnp.zeros(\n",
    "        (NUM_AGENTS, NUM_AGENTS), dtype=\"float32\"\n",
    "    ),  # second dim won't always be num_agents\n",
    "    \"action\": jnp.zeros(NUM_AGENTS, dtype=\"int32\"),\n",
    "    \"reward\": jnp.zeros(NUM_AGENTS, dtype=\"float32\"),\n",
    "    \"next_obs\": jnp.zeros((NUM_AGENTS, NUM_AGENTS), dtype=\"float32\"),\n",
    "    \"done\": 0.0,\n",
    "}  # store in dictionary\n",
    "\n",
    "q_learning_memory_state = q_learning_memory.init(transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_squared_error(pred, target):\n",
    "    \"\"\"A function to compute the mean-squared error between a prediction and a target value.\"\"\"\n",
    "\n",
    "    squared_error = jax.numpy.square(pred - target)\n",
    "\n",
    "    return squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bellman_target(reward, done, next_q_values, gamma=0.99):\n",
    "    \"\"\"A function to compute the bellman target.\"\"\"\n",
    "\n",
    "    bellman_target = reward + gamma * (1.0 - done) * jax.numpy.max(next_q_values)\n",
    "\n",
    "    return bellman_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_loss(q_values, action, reward, done, next_q_values):\n",
    "    \"\"\"Implementation of the Q-learning loss.\"\"\"\n",
    "\n",
    "    chosen_action_q_value = q_values[action]\n",
    "    bellman_target = compute_bellman_target(reward, done, next_q_values)\n",
    "    squared_error = compute_squared_error(chosen_action_q_value, bellman_target)\n",
    "\n",
    "    return squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agent_mse(\n",
    "    online_params, target_params, encoded_obs, actions, rewards, encoded_next_obs, dones\n",
    "):\n",
    "    \"\"\"A function to compute a single agent's mean-squared error.\"\"\"\n",
    "\n",
    "    q_values = q_network.apply(online_params, encoded_obs)  # use the online parameters\n",
    "    next_q_values = q_network.apply(\n",
    "        target_params, encoded_next_obs\n",
    "    )  # use the target parameters\n",
    "\n",
    "    # vmap the loss calculation over the batch\n",
    "    q_learning_loss_vmap = jax.vmap(q_learning_loss, in_axes=(0, 0, 0, 0, 0))\n",
    "    squared_error = q_learning_loss_vmap(\n",
    "        q_values, actions, rewards, dones, next_q_values\n",
    "    )\n",
    "\n",
    "    # Take the mean of the batch losses\n",
    "    mean_squared_error = jnp.mean(squared_error)\n",
    "\n",
    "    return mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_q_learning_loss(\n",
    "    online_params, target_params, obs, actions, rewards, next_obs, dones\n",
    "):\n",
    "    \"\"\"A function to compute the current and next Q-values and the squared loss over a batch, for both agents.\"\"\"\n",
    "\n",
    "    # Add one-hot encoding with agent IDs for each batch\n",
    "    agent_ids = jnp.repeat(\n",
    "        jnp.expand_dims(jnp.identity(NUM_AGENTS), axis=0), repeats=BATCH_SIZE, axis=0\n",
    "    )\n",
    "    encoded_obs = jnp.concatenate((obs, agent_ids), axis=2, dtype=\"float32\")\n",
    "    encoded_next_obs = jnp.concatenate((next_obs, agent_ids), axis=2, dtype=\"float32\")\n",
    "\n",
    "    # vmap the loss computation over both agents\n",
    "    compute_agent_mse_vmap = jax.vmap(\n",
    "        compute_agent_mse, in_axes=(None, None, 1, 1, 1, 1, None)\n",
    "    )\n",
    "    agent_mean_squared_errors = compute_agent_mse_vmap(\n",
    "        online_params,\n",
    "        target_params,\n",
    "        encoded_obs,\n",
    "        actions,\n",
    "        rewards,\n",
    "        encoded_next_obs,\n",
    "        dones,\n",
    "    )\n",
    "\n",
    "    # Take the mean between all agent MSEs are the loss value\n",
    "    loss_value = jnp.mean(agent_mean_squared_errors)\n",
    "\n",
    "    return loss_value  # returns a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Initialise Q-network optimiser\n",
    "OPTIMISER = optax.adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "Q_LEARN_OPTIM_STATE = OPTIMISER.init(q_network_params)  # initial optim state\n",
    "\n",
    "# Create Learn State\n",
    "q_learning_learn_state = QLearnState(\n",
    "    0, Q_LEARN_OPTIM_STATE\n",
    ")  # count set to zero initially\n",
    "\n",
    "# Add initial Q-network weights to QLearnParams object\n",
    "q_learning_params = QLearnParams(\n",
    "    online=q_network_params, target=q_network_params\n",
    ")  # target equal to online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_TARGET_PERIOD = 100  # how often to update the target network with the current online network parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_params(learn_state, online_weights, target_weights):\n",
    "    \"\"\"A function to update target params periodically.\"\"\"\n",
    "\n",
    "    target = jax.lax.cond(\n",
    "        jax.numpy.mod(learn_state.count, UPDATE_TARGET_PERIOD) == 0,\n",
    "        lambda x, y: x,\n",
    "        lambda x, y: y,\n",
    "        online_weights,\n",
    "        target_weights,\n",
    "    )\n",
    "\n",
    "    params = QLearnParams(online_weights, target)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn(params, learner_state, batch):\n",
    "    \"\"\"A function to perform Q-learning updates to the main network parameters, and maybe the to target network parameters.\"\"\"\n",
    "\n",
    "    # Compute gradients\n",
    "    grad_loss = jax.grad(batched_q_learning_loss)(\n",
    "        params.online,\n",
    "        params.target,\n",
    "        batch.experience[\"obs\"].astype(\"float32\"),\n",
    "        batch.experience[\"action\"].astype(\"int32\"),\n",
    "        batch.experience[\"reward\"].astype(\"float32\"),\n",
    "        batch.experience[\"next_obs\"].astype(\"float32\"),\n",
    "        batch.experience[\"done\"].astype(\"float32\"),\n",
    "    )\n",
    "\n",
    "    # Get updates\n",
    "    updates, opt_state = OPTIMISER.update(grad_loss, learner_state.optim_state)\n",
    "\n",
    "    # Apply them\n",
    "    new_weights = optax.apply_updates(params.online, updates)\n",
    "\n",
    "    # Maybe update target network\n",
    "    params = update_target_params(learner_state, new_weights, params.target)\n",
    "\n",
    "    # Increment learner step counter\n",
    "    learner_state = QLearnState(learner_state.count + 1, opt_state)\n",
    "\n",
    "    return params, learner_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the remaining training hyperparameters\n",
    "NUM_TIMESTEPS = 1001  # the total number of timesteps to take during training\n",
    "TRAINING_PERIOD = 4  # how often to train\n",
    "EVALUATION_EPISODES = 32  # how many evaluation episodes to run\n",
    "# Set NUM_ENVIRONMENTS = BATCH_SIZE to allow buffer to fill to the batch size on the first timestep\n",
    "NUM_ENVIRONMENTS = BATCH_SIZE  # the number of training environments to step through in parallel for experience accumulation\n",
    "EVALUATOR_PERIOD = 100  # how often to run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the scanning values for the training loop\n",
    "inner = jnp.array(TRAINING_PERIOD, dtype=\"int32\")\n",
    "middle = jnp.array(EVALUATOR_PERIOD / inner, dtype=\"int32\")\n",
    "outer = jnp.array((NUM_TIMESTEPS - 1) / (inner * middle), dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap the environments\n",
    "env_step_vmap = jax.vmap(env.step, in_axes=(0, 0))\n",
    "env_reset_vmap = jax.vmap(env.reset, in_axes=0)\n",
    "eval_env_step_vmap = jax.vmap(eval_env.step, in_axes=(0, 0))\n",
    "eval_env_reset_vmap = jax.vmap(eval_env.reset, in_axes=0)\n",
    "\n",
    "# vmap action selection across all agents and all environments\n",
    "select_all_agent_actions = jax.vmap(\n",
    "    q_learning_select_action, in_axes=(0, None, None, 0, None)\n",
    ")\n",
    "select_all_env_actions = jax.vmap(\n",
    "    select_all_agent_actions, in_axes=(0, None, None, 0, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_step_store(obs, env_state, agent_memory_state, key, agent_params, ts):\n",
    "    \"\"\"A function to select an action, take a step and store the observation during agent training.\"\"\"\n",
    "\n",
    "    # Add a one-hot encoding with the agent IDs for each environment\n",
    "    agent_ids = jnp.repeat(\n",
    "        jnp.expand_dims(jnp.identity(NUM_AGENTS), axis=0),\n",
    "        repeats=NUM_ENVIRONMENTS,\n",
    "        axis=0,\n",
    "    )\n",
    "    encoded_obs = jnp.concatenate((obs, agent_ids), axis=2, dtype=\"float32\")\n",
    "\n",
    "    # Select actions for each agent\n",
    "    key, subkey = jax.random.split(key)\n",
    "    subkeys = jax.random.split(subkey, num=(NUM_ENVIRONMENTS, NUM_AGENTS))\n",
    "    action = select_all_env_actions(subkeys, agent_params, ts, encoded_obs, False)\n",
    "\n",
    "    # Step the environment(s)\n",
    "    next_env_state, timestep = env_step_vmap(env_state, action)\n",
    "    next_obs = timestep.observation.agent_obs\n",
    "\n",
    "    # Add observations for each environment to the replay buffer\n",
    "    transition = {\n",
    "        \"obs\": obs * 1.0,\n",
    "        \"action\": action,\n",
    "        \"reward\": timestep.reward * 1.0,\n",
    "        \"next_obs\": next_obs * 1.0,\n",
    "        \"done\": 1 - timestep.discount,  # if terminated, done = True\n",
    "    }\n",
    "    # Add to the buffer\n",
    "    agent_memory_state = q_learning_memory.add(agent_memory_state, transition)\n",
    "\n",
    "    # Update obs and env state before next step\n",
    "    obs = next_obs\n",
    "    env_state = next_env_state\n",
    "\n",
    "    # Increment the timestep\n",
    "    ts = ts + 1\n",
    "\n",
    "    return obs, env_state, agent_memory_state, key, ts\n",
    "\n",
    "\n",
    "def action_step_store_scan(obs_env_mem_key_ts, _):\n",
    "    \"\"\"A scan-compatible version of action_step_store.\"\"\"\n",
    "\n",
    "    # Unpack the initial state\n",
    "    obs, env_state, agent_memory_state, key, agent_params, ts = obs_env_mem_key_ts\n",
    "\n",
    "    # Perform action, step, and store\n",
    "    obs, env_state, agent_memory_state, key, ts = action_step_store(\n",
    "        obs, env_state, agent_memory_state, key, agent_params, ts\n",
    "    )\n",
    "\n",
    "    # Re-pack the updated state\n",
    "    obs_env_mem_key_ts = (obs, env_state, agent_memory_state, key, agent_params, ts)\n",
    "\n",
    "    return obs_env_mem_key_ts, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(agent_params, agent_learner_state, key, agent_memory_state):\n",
    "    \"\"\"A function perform a learning step during agent training.\"\"\"\n",
    "\n",
    "    # Generate a new key\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # First sample memory and then pass the result to the learn function\n",
    "    batch = q_learning_memory.sample(agent_memory_state, subkey)\n",
    "    agent_params, agent_learner_state = q_learn(\n",
    "        agent_params, agent_learner_state, batch\n",
    "    )\n",
    "\n",
    "    return agent_params, agent_learner_state, key\n",
    "\n",
    "\n",
    "def learn_scan(learn_state, _):\n",
    "    \"\"\"A nested, scan-compatible version of learn and action_step_store.\"\"\"\n",
    "\n",
    "    # Unpack the initial state for learn\n",
    "    (\n",
    "        agent_params,\n",
    "        agent_learner_state,\n",
    "        key,\n",
    "        agent_memory_state,\n",
    "        obs,\n",
    "        env_state,\n",
    "        ts,\n",
    "    ) = learn_state\n",
    "\n",
    "    # Perform parameter updates\n",
    "    agent_params, agent_learner_state, key = learn(\n",
    "        agent_params, agent_learner_state, key, agent_memory_state\n",
    "    )\n",
    "\n",
    "    # Define the initial state for action_step_store_scan\n",
    "    action_step_store_state = (\n",
    "        obs,\n",
    "        env_state,\n",
    "        agent_memory_state,\n",
    "        key,\n",
    "        agent_params,\n",
    "        ts,\n",
    "    )\n",
    "    # Perform scan for taking an action, stepping and storing in the buffer\n",
    "    action_step_store_state, _ = jax.lax.scan(\n",
    "        action_step_store_scan, action_step_store_state, xs=None, length=inner\n",
    "    )\n",
    "    # Unpack arguments from updated action_step_store_scan state\n",
    "    obs, env_state, agent_memory_state, key, agent_params, ts = action_step_store_state\n",
    "\n",
    "    # Repack the updated state for learn_scan\n",
    "    learn_state = (\n",
    "        agent_params,\n",
    "        agent_learner_state,\n",
    "        key,\n",
    "        agent_memory_state,\n",
    "        obs,\n",
    "        env_state,\n",
    "        ts,\n",
    "    )\n",
    "\n",
    "    return learn_state, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ts, key, agent_params):\n",
    "    \"\"\"A function to perform evaluation on the learned policy during training.\"\"\"\n",
    "\n",
    "    # Since an episode is one step in this environment, we can evaluate the policy using a single step!\n",
    "\n",
    "    # Reset the environments\n",
    "    key, subkey = jax.random.split(key)\n",
    "    subkeys = jax.random.split(subkey, num=EVALUATION_EPISODES)\n",
    "    eval_env_state, eval_timestep = eval_env_reset_vmap(subkeys)\n",
    "    eval_obs = eval_timestep.observation.agent_obs\n",
    "\n",
    "    # Add a one-hot encoding with the agent IDs for the NN\n",
    "    agent_ids = jnp.repeat(\n",
    "        jnp.expand_dims(jnp.identity(NUM_AGENTS), axis=0),\n",
    "        repeats=EVALUATION_EPISODES,\n",
    "        axis=0,\n",
    "    )\n",
    "    encoded_eval_obs = jnp.concatenate((eval_obs, agent_ids), axis=2, dtype=\"float32\")\n",
    "\n",
    "    # Select actions for each agent\n",
    "    key, subkey = jax.random.split(key)\n",
    "    subkeys = jax.random.split(subkey, num=(EVALUATION_EPISODES, NUM_AGENTS))\n",
    "    eval_action = select_all_env_actions(\n",
    "        subkeys, agent_params, ts, encoded_eval_obs, True\n",
    "    )\n",
    "\n",
    "    # Step the environment(s)\n",
    "    eval_env_state, eval_timestep = eval_env_step_vmap(eval_env_state, eval_action)\n",
    "    eval_obs = eval_timestep.observation.agent_obs\n",
    "\n",
    "    # Record the average return over all (single step) evaluation episodes\n",
    "    evaluator_return = jnp.mean(eval_timestep.reward * 1.0, axis=0)\n",
    "\n",
    "    return evaluator_return, ts, key\n",
    "\n",
    "\n",
    "def evaluate_scan(evaluate_state, _):\n",
    "    \"\"\"A nested, scan-compatible version of evaluate, learn and action_step_store.\"\"\"\n",
    "\n",
    "    # Unpack the initial state for evaluate\n",
    "    (\n",
    "        evaluator_return,\n",
    "        ts,\n",
    "        key,\n",
    "        agent_params,\n",
    "        agent_learner_state,\n",
    "        agent_memory_state,\n",
    "        obs,\n",
    "        env_state,\n",
    "        ts,\n",
    "    ) = evaluate_state\n",
    "\n",
    "    # Evaluate the learned policy\n",
    "    evaluator_return, ts, key = evaluate(ts, key, agent_params)\n",
    "\n",
    "    # Define the shared initial state for learn_scan\n",
    "    learn_state = (\n",
    "        agent_params,\n",
    "        agent_learner_state,\n",
    "        key,\n",
    "        agent_memory_state,\n",
    "        obs,\n",
    "        env_state,\n",
    "        ts,\n",
    "    )\n",
    "    # Perform scan for learning and taking an action, stepping and storing in the buffer\n",
    "    learn_state, _ = jax.lax.scan(learn_scan, learn_state, xs=None, length=middle)\n",
    "    # Unpack arguments from updated learn_scan state\n",
    "    (\n",
    "        agent_params,\n",
    "        agent_learner_state,\n",
    "        key,\n",
    "        agent_memory_state,\n",
    "        obs,\n",
    "        env_state,\n",
    "        ts,\n",
    "    ) = learn_state\n",
    "\n",
    "    # Repack the updated state for evaluate_scan\n",
    "    evaluate_state = (\n",
    "        evaluator_return,\n",
    "        ts,\n",
    "        key,\n",
    "        agent_params,\n",
    "        agent_learner_state,\n",
    "        agent_memory_state,\n",
    "        obs,\n",
    "        env_state,\n",
    "        ts,\n",
    "    )\n",
    "\n",
    "    return evaluate_state, evaluator_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_loop(agent_params, agent_learner_state, agent_memory_state, seed=42):\n",
    "    \"\"\"\n",
    "    This function runs several episodes in an environment and periodically\n",
    "    does some agent learning and evaluation.\n",
    "\n",
    "    Args:\n",
    "        agent_params: an object to store parameters that the agent uses.\n",
    "        agent_learner_state: an object that stores the internal state\n",
    "            of the agent learn function.\n",
    "        agent_memory_state: an object that stores the internal state of the\n",
    "            agent memory.\n",
    "        seed: PRNG seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        evaluator_episode_returns: list of all the evaluator episode returns\n",
    "        for each agent.\n",
    "    \"\"\"\n",
    "\n",
    "    # JAX random number generator\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "\n",
    "    # Reset the environment(s)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    subkeys = jax.random.split(subkey, num=NUM_ENVIRONMENTS)\n",
    "    env_state, timestep = env_reset_vmap(subkeys)\n",
    "    obs = timestep.observation.agent_obs\n",
    "\n",
    "    # Set the timestep counter to zero\n",
    "    ts = 0\n",
    "\n",
    "    # Initialise the average return variable for evaluation in evaluate_scan\n",
    "    evaluator_return = jnp.array([0.0, 0.0])\n",
    "\n",
    "    # Pack arguments into a state for scanning over evaluate\n",
    "    evaluate_state = (\n",
    "        evaluator_return,\n",
    "        ts,\n",
    "        key,\n",
    "        agent_params,\n",
    "        agent_learner_state,\n",
    "        agent_memory_state,\n",
    "        obs,\n",
    "        env_state,\n",
    "        ts,\n",
    "    )\n",
    "    # Perform scan over evaluate, learn and action_step_store\n",
    "    evaluate_state, evaluator_returns = jax.lax.scan(\n",
    "        evaluate_scan, evaluate_state, xs=None, length=outer\n",
    "    )\n",
    "    # Unpack arguments from the updated state\n",
    "    (\n",
    "        evaluator_return,\n",
    "        ts,\n",
    "        key,\n",
    "        agent_params,\n",
    "        agent_learner_state,\n",
    "        agent_memory_state,\n",
    "        obs,\n",
    "        env_state,\n",
    "        ts,\n",
    "    ) = evaluate_state\n",
    "\n",
    "    # Evaluate the final learned policy at the end of training\n",
    "    final_evaluator_return, ts, key = evaluate(ts, key, agent_params)\n",
    "\n",
    "    evaluator_returns = jnp.append(\n",
    "        evaluator_returns, jnp.expand_dims(final_evaluator_return, axis=0), axis=0\n",
    "    )\n",
    "\n",
    "    return evaluator_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training_loop_jitted = jax.jit(run_training_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent and evaluate\n",
    "\n",
    "Below, we print the results for the evaluation rounds for each of the two agents as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 0     \tAgent 1 Return: 2.00\tAgent 2 Return: 2.00\n",
      "Timestep: 100   \tAgent 1 Return: 2.00\tAgent 2 Return: 2.00\n",
      "Timestep: 200   \tAgent 1 Return: 4.00\tAgent 2 Return: 4.00\n",
      "Timestep: 300   \tAgent 1 Return: 4.00\tAgent 2 Return: 4.00\n",
      "Timestep: 400   \tAgent 1 Return: 4.00\tAgent 2 Return: 4.00\n",
      "Timestep: 500   \tAgent 1 Return: 4.00\tAgent 2 Return: 4.00\n",
      "Timestep: 600   \tAgent 1 Return: 4.00\tAgent 2 Return: 4.00\n",
      "Timestep: 700   \tAgent 1 Return: 4.00\tAgent 2 Return: 4.00\n",
      "Timestep: 800   \tAgent 1 Return: 4.00\tAgent 2 Return: 4.00\n",
      "Timestep: 900   \tAgent 1 Return: 4.00\tAgent 2 Return: 4.00\n",
      "Timestep: 1000  \tAgent 1 Return: 4.00\tAgent 2 Return: 4.00\n"
     ]
    }
   ],
   "source": [
    "# Run environment loop\n",
    "evaluator_returns = run_training_loop_jitted(\n",
    "    q_learning_params, q_learning_learn_state, q_learning_memory_state, seed=SEED\n",
    ")\n",
    "\n",
    "# Generate timesteps from 0 to NUM_TIMESTEPS with a step of EVALUATOR_PERIOD\n",
    "timesteps = jnp.arange(0, NUM_TIMESTEPS, EVALUATOR_PERIOD)\n",
    "\n",
    "# Create logs for each timestep and its corresponding average return\n",
    "logs = [\n",
    "    f\"Timestep: {str(timestep).ljust(6, ' ')}\\tAgent 1 Return: {avg_return[0]:.2f}\\tAgent 2 Return: {avg_return[1]:.2f}\"\n",
    "    for timestep, avg_return in zip(timesteps, evaluator_returns)\n",
    "]\n",
    "\n",
    "# Print the logs\n",
    "print(*logs, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both agents have converged to their optimal return in their payoff matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on IQL Performance in Matrix Games\n",
    "\n",
    "In some Matrax environments, IQL often falls into local optima. One such example of this can be seen using Climbing Game. Agents are expected to cooperately reach the highest reward of 11 by taking a joint action that risks a large punishment if either agent deviates from the optimal joint action. However, each agent is acting independently to optimise its own returns, creating a scenario where the agents are reluctant to frequently take suboptimal actions that may yield a higher joint reward.\n",
    "\n",
    "See this paper for more details:\n",
    "\n",
    "[Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2102.04402.pdf#:~:text=Thus%2C%20for%20decentralized%20policy%20learning,less%20bias%20and%20more%20variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
